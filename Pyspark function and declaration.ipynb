{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a928c8-0444-4b89-ba52-c897598966b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    (1, \"Name_1\", 34, 45820, \"HR\", \"2018-03-15\"),\n",
    "    (2, \"Name_2\", 45, 78000, \"IT\", \"2019-06-07\"),\n",
    "    (3, \"Name_3\", 29, 62000, \"Sales\", \"2020-11-22\"),\n",
    "    (4, \"Name_4\", 38, 54000, \"Finance\", \"2017-08-30\"),\n",
    "    (5, \"Name_5\", 41, 95000, \"IT\", \"2015-02-10\"),\n",
    "    (6, \"Name_6\", 55, None , \"HR\", \"2013-04-12\"),\n",
    "    (7, \"Name_7\", 26, 67000, \"Sales\", \"2021-01-06\"),\n",
    "    (8, \"Name_8\", 50, 90000, \"Finance\", \"2016-07-19\"),\n",
    "    (9, \"Name_9\", 60, None, \"IT\", \"2014-10-11\"),\n",
    "    (10, \"Name_10\", 35, 49000, \"HR\", \"2018-12-18\"),\n",
    "    (11, \"Name_11\", 28, 53000, \"Finance\", \"2019-05-25\"),\n",
    "    (12, \"Name_12\", 40, 75000, \"IT\", \"2017-03-03\"),\n",
    "    (13, \"Name_13\", 52, 89000, \"Sales\", \"2013-09-14\"),\n",
    "    (14, \"Name_14\", 27, 66000, \"HR\", \"2022-08-21\"),\n",
    "    (15, \"Name_15\", 33, 50000, \"Finance\", \"2020-02-13\"),\n",
    "    (16, \"Name_16\", 44, 72000, \"IT\", \"2014-06-25\"),\n",
    "    (17, \"Name_17\", 30, 80000, \"Sales\", \"2021-07-30\"),\n",
    "    (18, \"Name_18\", 49, 93000, \"HR\", \"2016-05-09\"),\n",
    "    (19, \"Name_19\", 36, 64000, \"Finance\", \"2019-12-02\"),\n",
    "    (20, \"Name_20\", 31, 70000, \"IT\", \"2020-09-13\")\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "columns = [\"ID\", \"Name\", \"Age\", \"Salary\", \"Department\", \"JoinDate\"]\n",
    "\n",
    "# Create the dataframe\n",
    "df = spark.createDataFrame(data, columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c0c36ec-d20e-4c6b-9028-17f2f8311bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### `df.show()` Command in PySpark\n",
    "\n",
    "The `df.show()` function is used to display the first few rows of a DataFrame in PySpark. It is commonly used to quickly inspect the content of a DataFrame.\n",
    "\n",
    "#### Syntax:\n",
    " **`df.show(n=20, truncate=True)`**\n",
    "## Parameters:\n",
    "\n",
    "- **n** (optional, default=20):\n",
    "  - Specifies the number of rows to display.\n",
    "  - By default, it displays the first 20 rows of the DataFrame.\n",
    "  - If you want to display a different number of rows, you can pass an integer to the `n` parameter.\n",
    "\n",
    "- **truncate** (optional, default=True):\n",
    "  - If set to `True` (default), it truncates long strings to a maximum of 20 characters for each column.\n",
    "  - If set to `False`, it shows the full content of each string, no matter how long the text is.\n",
    "  - This is particularly useful when you want to inspect data with very long text fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1640f817-3536-4676-b88f-79d426444bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+----------+----------+\n| ID|   Name|Age|Salary|Department|  JoinDate|\n+---+-------+---+------+----------+----------+\n|  1| Name_1| 34| 45820|        HR|2018-03-15|\n|  2| Name_2| 45| 78000|        IT|2019-06-07|\n|  3| Name_3| 29| 62000|     Sales|2020-11-22|\n|  4| Name_4| 38| 54000|   Finance|2017-08-30|\n|  5| Name_5| 41| 95000|        IT|2015-02-10|\n|  6| Name_6| 55|120000|        HR|2013-04-12|\n|  7| Name_7| 26| 67000|     Sales|2021-01-06|\n|  8| Name_8| 50| 90000|   Finance|2016-07-19|\n|  9| Name_9| 60|110000|        IT|2014-10-11|\n| 10|Name_10| 35| 49000|        HR|2018-12-18|\n| 11|Name_11| 28| 53000|   Finance|2019-05-25|\n| 12|Name_12| 40| 75000|        IT|2017-03-03|\n| 13|Name_13| 52| 89000|     Sales|2013-09-14|\n| 14|Name_14| 27| 66000|        HR|2022-08-21|\n| 15|Name_15| 33| 50000|   Finance|2020-02-13|\n| 16|Name_16| 44| 72000|        IT|2014-06-25|\n| 17|Name_17| 30| 80000|     Sales|2021-07-30|\n| 18|Name_18| 49| 93000|        HR|2016-05-09|\n| 19|Name_19| 36| 64000|   Finance|2019-12-02|\n| 20|Name_20| 31| 70000|        IT|2020-09-13|\n+---+-------+---+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Default - displays 20 rows and \n",
    "# 20 charactes from column value \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2d5bed-0b1b-42f5-837a-c8bed82b3cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+----------+----------+\n|ID |Name   |Age|Salary|Department|JoinDate  |\n+---+-------+---+------+----------+----------+\n|1  |Name_1 |34 |45820 |HR        |2018-03-15|\n|2  |Name_2 |45 |78000 |IT        |2019-06-07|\n|3  |Name_3 |29 |62000 |Sales     |2020-11-22|\n|4  |Name_4 |38 |54000 |Finance   |2017-08-30|\n|5  |Name_5 |41 |95000 |IT        |2015-02-10|\n|6  |Name_6 |55 |120000|HR        |2013-04-12|\n|7  |Name_7 |26 |67000 |Sales     |2021-01-06|\n|8  |Name_8 |50 |90000 |Finance   |2016-07-19|\n|9  |Name_9 |60 |110000|IT        |2014-10-11|\n|10 |Name_10|35 |49000 |HR        |2018-12-18|\n|11 |Name_11|28 |53000 |Finance   |2019-05-25|\n|12 |Name_12|40 |75000 |IT        |2017-03-03|\n|13 |Name_13|52 |89000 |Sales     |2013-09-14|\n|14 |Name_14|27 |66000 |HR        |2022-08-21|\n|15 |Name_15|33 |50000 |Finance   |2020-02-13|\n|16 |Name_16|44 |72000 |IT        |2014-06-25|\n|17 |Name_17|30 |80000 |Sales     |2021-07-30|\n|18 |Name_18|49 |93000 |HR        |2016-05-09|\n|19 |Name_19|36 |64000 |Finance   |2019-12-02|\n|20 |Name_20|31 |70000 |IT        |2020-09-13|\n+---+-------+---+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Display full column contents\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e295c3a7-0053-424e-b97e-c2e6881330e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+----------+----------+\n|ID |Name  |Age|Salary|Department|JoinDate  |\n+---+------+---+------+----------+----------+\n|1  |Name_1|34 |45820 |HR        |2018-03-15|\n|2  |Name_2|45 |78000 |IT        |2019-06-07|\n+---+------+---+------+----------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Display 2 rows and full column contents\n",
    "df.show(2,truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef88dd8c-9221-45a5-b546-9866dcff96a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+----------+----------+\n| ID|  Name|Age|Salary|Department|  JoinDate|\n+---+------+---+------+----------+----------+\n|  1|Name_1| 34| 45820|        HR|2018-03-15|\n|  2|Name_2| 45| 78000|        IT|2019-06-07|\n+---+------+---+------+----------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Display 2 rows & column values 25 characters\n",
    "df.show(2,truncate=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb731d23-42b2-4269-8e28-9dad26308b95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------\n ID         | 1          \n Name       | Name_1     \n Age        | 34         \n Salary     | 45820      \n Department | HR         \n JoinDate   | 2018-03-15 \n-RECORD 1----------------\n ID         | 2          \n Name       | Name_2     \n Age        | 45         \n Salary     | 78000      \n Department | IT         \n JoinDate   | 2019-06-07 \n-RECORD 2----------------\n ID         | 3          \n Name       | Name_3     \n Age        | 29         \n Salary     | 62000      \n Department | Sales      \n JoinDate   | 2020-11-22 \nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Display DataFrame rows & columns vertically\n",
    "df.show(n=3,truncate=25,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9b7f549-573e-446a-a303-57d36e5bb81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b8e95e-977f-4a44-ac80-3c4fbe86c290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining StructType and StructField \n",
    "schema=StructType([StructField(\"ID\",IntegerType(),True),\\\n",
    "  StructField(\"Name\",StringType(),True),\\\n",
    "  StructField(\"Age\",IntegerType(),True),\\\n",
    "  StructField(\"Salary\",IntegerType(),True),\\\n",
    "  StructField(\"Department\",StringType(),True),\\\n",
    "  StructField(\"JoinDate\",StringType(),True)])\n",
    "df=spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dc61b82-80a8-4ed0-b214-3b2ee943cd1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426d3a17-cd35-4c57-bd2e-58cdab4679c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n|  Name|Salary|Department|\n+------+------+----------+\n|Name_1| 45820|        HR|\n|Name_2| 78000|        IT|\n+------+------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining Col function\n",
    "df1=df.select(col(\"Name\"),col(\"Salary\"),col(\"Department\"))\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8c1079b-7ed7-4bfb-b61b-b9e94fec3b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ed1784-01af-4bdd-97cb-b37ac2ccb027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n|  Name|Salary|Department|\n+------+------+----------+\n|Name_1| 45820|        HR|\n|Name_2| 78000|        IT|\n+------+------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining select function\n",
    "df1=df.select(df.Name,col(\"Salary\"),\"Department\")\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6f3b3d4-f350-4764-af21-b6dfd3ed60a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd0d054-cadb-4e56-aaa3-ef870f4dd98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name_1\nName_2\nName_3\nName_4\nName_5\nName_6\nName_7\nName_8\nName_9\nName_10\nName_11\nName_12\nName_13\nName_14\nName_15\nName_16\nName_17\nName_18\nName_19\nName_20\n"
     ]
    }
   ],
   "source": [
    "#defining collect function\n",
    "df1=df.select(\"Name\",col(\"Salary\"),df.Department)\n",
    "#normal collect which will collect all the rows\n",
    "df1=df1.collect()\n",
    "for i in df1:\n",
    "  print(i['Name'])\n",
    "# if we have to collect the any specific row using collect \n",
    "df1=df1.collect()[3]\n",
    "# if we have to collect the any specific value from a specific row and specific column using collect \n",
    "df1=df1.collect()[3][0]\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a4a3296-95b9-4bc5-b62b-c6c3d9711284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47aeb3cc-15a9-4499-9f46-109bb1a4d332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+----------+----------+----------+\n| ID|  Name|Age|Salary|Department|  JoinDate|New_column|\n+---+------+---+------+----------+----------+----------+\n|  1|Name_1| 34| 45820|        HR|2018-03-15|     15273|\n|  2|Name_2| 45| 78000|        IT|2019-06-07|     26000|\n+---+------+---+------+----------+----------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining withColumn function\n",
    "# adding new column\n",
    "df1=df.withColumn(\"New_column\",lit(\"added_row\"))\n",
    "#changing the data type of existing column\n",
    "df1=df.withColumn(\"JoinDate\",col(\"JoinDate\").cast(DateType()))\n",
    "#deriving a new column from existing column\n",
    "df1=df.withColumn(\"New_column\",(df.Salary/3).cast(DecimalType()))\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c3e79c4-78dc-4d4d-bda8-39d40b121cec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bcf8d91-19b2-4ff6-bee9-3c62c877c9d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+\n|  Name|Salary|Department|\n+------+------+----------+\n|Name_1| 45820|        HR|\n|Name_2| 78000|        IT|\n+------+------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining drop function\n",
    "df1=df.drop(col(\"JoinDate\"),\"ID\",df.Age)\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c79ad365-47b1-4382-96be-0e64e46c1d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9c180c-4fd1-43e1-bea7-fa61d4186107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+--------+----------+----------+\n| ID|  Name|Age|Salary_1|Department|  JoinDate|\n+---+------+---+--------+----------+----------+\n|  1|Name_1| 34|   45820|        HR|2018-03-15|\n|  2|Name_2| 45|   78000|        IT|2019-06-07|\n+---+------+---+--------+----------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining withcolumnRename function\n",
    "df1=df.withColumnRenamed(\"Salary\",\"Salary_1\")\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3e8f360-1eed-492b-ba19-dfe784455164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+----------+----------+\n| ID|  Name|Age|Salary|Department|  JoinDate|\n+---+------+---+------+----------+----------+\n|  1|Name_1| 34| 45820|        HR|2018-03-15|\n|  2|Name_2| 45| 78000|        IT|2019-06-07|\n+---+------+---+------+----------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining Filter function\n",
    "df1=df.filter(df.Salary>=40000)\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1d686bd-8118-4006-8f12-95a36b8dda11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf668e2-9a50-4210-b92b-de5afe0a86ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+----------+----------+\n| ID|  Name|Age|Salary|Department|  JoinDate|\n+---+------+---+------+----------+----------+\n|  1|Name_1| 34| 45820|        HR|2018-03-15|\n|  2|Name_2| 45| 78000|        IT|2019-06-07|\n+---+------+---+------+----------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining Where function\n",
    "#pyspark way of defining\n",
    "df1=df.where(df.Salary>=40000)\n",
    "#sql way for defining \n",
    "df1=df.where(\"salary>=40000\")\n",
    "# filter with multiple condition \n",
    "df1=df.filter(((df.Name!=\"abc\")&(df.Name!=\"abc\"))|((df.Name!=\"abc\")&(df.Name!=\"abc\")))\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae6fcc3-d5e6-42f3-a1b3-5279002e25b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+----------+----------+\n| ID|  Name|Age|Salary|Department|  JoinDate|\n+---+------+---+------+----------+----------+\n|  2|Name_2| 45| 78000|        IT|2019-06-07|\n|  3|Name_3| 29| 62000|     Sales|2020-11-22|\n+---+------+---+------+----------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# filter condition with isnull function\n",
    "df1=df.filter(col(\"Name\").isNull())\n",
    "# filter condition with isnotnull function\n",
    "df1=df.filter(col(\"Name\").isNotNull())\n",
    "# filter condition with startwith function\n",
    "df1=df.filter(col(\"Name\").startswith(\"James\"))\n",
    "# filter condition with doesnot startwith function\n",
    "df1=df.filter(~col(\"Name\").startswith(\"abc\"))\n",
    "# filter condition with endwith function\n",
    "df1=df.filter(df.Name.endswith(\"abc\"))\n",
    "# filter condition with doesnot endswith function\n",
    "df1=df.filter(~df.Name.endswith(\"abc\"))\n",
    "# filter condition with contains\n",
    "df1=df.filter(col(\"Name\").contains(\"ame\")) # it is  case sensitive so have to give proper a vlue like \"ame\" to validate \n",
    "# filter condition with like \n",
    "df1 = df.filter(df.Name.like(\"%ame%\"))# it is also case sensitive as contains\n",
    "# filter condition with rlike \n",
    "df1=df.filter(df.Name.rlike(\"ame\"))\n",
    "# filter condition with in \n",
    "df1=df.filter(df.Age.isin(34)) \n",
    "# filter condition with in whihc has multiple values\n",
    "l1=[34,45] \n",
    "df1=df.filter(df.Age.isin(l1))\n",
    "# filter condition with not in \n",
    "df1=df.filter(~df.Age.isin(34)) \n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba631d6b-f7dd-4f4e-b64f-73678b126dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ab02cc-c97e-43de-b2e8-54706fd4798e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+----------+----------+\n| ID|  Name|Age|Salary|Department|  JoinDate|\n+---+------+---+------+----------+----------+\n|  2|Name_2| 45| 78000|        IT|2019-06-07|\n|  1|Name_1| 34| 45820|        HR|2018-03-15|\n+---+------+---+------+----------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining distinct function\n",
    "df1=df.distinct()\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a41f5a2e-0c25-4fe1-b60b-8c88084e7cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6153c862-a126-4cb3-8a63-6b7212d8c68f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+----------+----------+\n| ID|   Name|Age|Salary|Department|  JoinDate|\n+---+-------+---+------+----------+----------+\n|  1| Name_1| 34| 45820|        HR|2018-03-15|\n| 10|Name_10| 35| 49000|        HR|2018-12-18|\n+---+-------+---+------+----------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining dropDuplicates function\n",
    "df1 = df.dropDuplicates([\"Name\",\"Salary\"]) # drop duplicates helps in getting distinct values based on specific columns\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d561b2a1-616d-4bdc-8be1-69fa4f150894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ef63a5-ffd2-4130-b4ba-48d5fca54d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+----------+----------+\n| ID|   Name|Age|Salary|Department|  JoinDate|\n+---+-------+---+------+----------+----------+\n|  7| Name_7| 26| 67000|     Sales|2021-01-06|\n| 14|Name_14| 27| 66000|        HR|2022-08-21|\n| 11|Name_11| 28| 53000|   Finance|2019-05-25|\n|  3| Name_3| 29| 62000|     Sales|2020-11-22|\n| 17|Name_17| 30| 80000|     Sales|2021-07-30|\n+---+-------+---+------+----------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining sort function\n",
    "df1=df.sort(\"Age\",desc(col(\"Name\")))\n",
    "df1=df.sort(col(\"Age\").asc(),col(\"Name\").desc())\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "388c1c93-69e2-4604-9d85-25c1dcd084b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4de0a7-08a0-4e8e-b905-523945bd8465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+----------+----------+\n| ID|   Name|Age|Salary|Department|  JoinDate|\n+---+-------+---+------+----------+----------+\n|  7| Name_7| 26| 67000|     Sales|2021-01-06|\n| 14|Name_14| 27| 66000|        HR|2022-08-21|\n| 11|Name_11| 28| 53000|   Finance|2019-05-25|\n|  3| Name_3| 29| 62000|     Sales|2020-11-22|\n| 17|Name_17| 30| 80000|     Sales|2021-07-30|\n+---+-------+---+------+----------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#defining orderBy function\n",
    "df1=df.orderBy(\"Age\",desc(col(\"Name\")))\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26bc695e-835d-41f9-b30e-1dcd926cafd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057484ac-d9ef-491d-89ad-860993cfc869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#defining groupby function\n",
    "df1=df.groupBy(col(\"Age\")).agg(sum(\"Salary\").alias(\"sum value\"))\n",
    "#multiple columns aggrigartion with same grouping data \n",
    "df1=df.groupBy(col(\"Age\")).agg(sum(col(\"Salary\")).alias(\"sum\"),avg(col(\"Salary\")).alias(\"avg\"))\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ae9fbc2-3360-4d90-ac77-1e0d7e2faf09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd37d978-d4fa-47e6-a319-0ad9dbd3530f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    Row(ID=1, Department=\"HR\", Total_Salary=370000, Average_Salary=61666.7),\n",
    "    Row(ID=2, Department=\"ITI\", Total_Salary=564000, Average_Salary=70500.0),\n",
    "    Row(ID=3, Department=\"Sales\", Total_Salary=335000, Average_Salary=67000.0),\n",
    "    Row(ID=4, Department=\"Finance\", Total_Salary=365000, Average_Salary=61000.0),\n",
    "    Row(ID=5, Department=\"Support\", Total_Salary=35000, Average_Salary=71600.0)\n",
    "]\n",
    "\n",
    "# Create DataFrame from the list of rows\n",
    "s_df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fa3598-9264-4d79-a088-7a0adbabce30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+----------+----------+\n| ID|   Name|Age|Salary|Department|  JoinDate|\n+---+-------+---+------+----------+----------+\n|  2| Name_2| 45| 78000|        IT|2019-06-07|\n|  5| Name_5| 41| 95000|        IT|2015-02-10|\n|  9| Name_9| 60|110000|        IT|2014-10-11|\n| 12|Name_12| 40| 75000|        IT|2017-03-03|\n| 16|Name_16| 44| 72000|        IT|2014-06-25|\n| 20|Name_20| 31| 70000|        IT|2020-09-13|\n+---+-------+---+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#defining Join function\n",
    "df1=df\n",
    "df2=s_df\n",
    "join_df=df1.join(df2,df1.Department==df2.Department,\"inner\")\n",
    "# want to print only specifi columns from tables \n",
    "join_df=df1.join(df2,df1.Department==df2.Department,\"inner\").select(df1.Name,df1.Age,df2.Department,df2.Average_Salary)\n",
    "#left Join\n",
    "join_df=df1.join(df2,df1.Department==df2.Department,\"left\")\n",
    "#right Join\n",
    "join_df=df1.join(df2,df1.Department==df2.Department,\"right\")\n",
    "#outer join\n",
    "join_df=df1.join(df2,df1.Department==df2.Department,\"outer\")\n",
    "#left semi join\n",
    "join_df=df1.join(df2,df1.Department==df2.Department,\"leftSemi\") #gives left side match and non null values\n",
    "#left anti\n",
    "join_df=df1.join(df2,df1.Department==df2.Department,\"leftanti\")#gives left side not match and non null values\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5107f640-8ff5-4256-877c-3c4f139b5439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebde81bc-2f42-4c78-b003-8218b06e6509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Name_1\", 34, 45820, \"HR\", \"2018-03-15\"),\n",
    "    (2, \"Name_2\", 45, 78000, \"IT\", \"2019-06-07\"),\n",
    "    (3, \"Name_3\", 29, 62000, \"Sales\", \"2020-11-22\")\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "columns = [\"ID\", \"Name\", \"Age\", \"Salary\", \"Department\", \"JoinDate\"]\n",
    "\n",
    "# Create the dataframe\n",
    "u_df = spark.createDataFrame(data, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237b0a47-2f94-4e5a-9cf9-5ad48a886f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+----------+----------+\n| ID|   Name|Age|Salary|Department|  JoinDate|\n+---+-------+---+------+----------+----------+\n|  2| Name_2| 45| 78000|        IT|2019-06-07|\n|  1| Name_1| 34| 45820|        HR|2018-03-15|\n|  4| Name_4| 38| 54000|   Finance|2017-08-30|\n|  3| Name_3| 29| 62000|     Sales|2020-11-22|\n|  6| Name_6| 55|120000|        HR|2013-04-12|\n|  5| Name_5| 41| 95000|        IT|2015-02-10|\n|  7| Name_7| 26| 67000|     Sales|2021-01-06|\n|  9| Name_9| 60|110000|        IT|2014-10-11|\n| 10|Name_10| 35| 49000|        HR|2018-12-18|\n|  8| Name_8| 50| 90000|   Finance|2016-07-19|\n| 12|Name_12| 40| 75000|        IT|2017-03-03|\n| 11|Name_11| 28| 53000|   Finance|2019-05-25|\n| 14|Name_14| 27| 66000|        HR|2022-08-21|\n| 13|Name_13| 52| 89000|     Sales|2013-09-14|\n| 15|Name_15| 33| 50000|   Finance|2020-02-13|\n| 16|Name_16| 44| 72000|        IT|2014-06-25|\n| 19|Name_19| 36| 64000|   Finance|2019-12-02|\n| 18|Name_18| 49| 93000|        HR|2016-05-09|\n| 20|Name_20| 31| 70000|        IT|2020-09-13|\n| 17|Name_17| 30| 80000|     Sales|2021-07-30|\n+---+-------+---+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#defining Union function\n",
    "union_df=df.union(u_df).distinct() # union and union all works same in pyspark so both can have duplicates so we have to use distinct to handle the duplicates\n",
    "union_df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321b7e81-3ecb-4a91-8ffa-0bd91b9e17c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+----------+----------+\n| ID|   Name|Age|Salary|Department|  JoinDate|\n+---+-------+---+------+----------+----------+\n|  1| Name_1| 34| 45820|        HR|2018-03-15|\n|  2| Name_2| 45| 78000|        IT|2019-06-07|\n|  3| Name_3| 29| 62000|     Sales|2020-11-22|\n|  4| Name_4| 38| 54000|   Finance|2017-08-30|\n|  5| Name_5| 41| 95000|        IT|2015-02-10|\n|  6| Name_6| 55|120000|        HR|2013-04-12|\n|  7| Name_7| 26| 67000|     Sales|2021-01-06|\n|  8| Name_8| 50| 90000|   Finance|2016-07-19|\n|  9| Name_9| 60|110000|        IT|2014-10-11|\n| 10|Name_10| 35| 49000|        HR|2018-12-18|\n| 11|Name_11| 28| 53000|   Finance|2019-05-25|\n| 12|Name_12| 40| 75000|        IT|2017-03-03|\n| 13|Name_13| 52| 89000|     Sales|2013-09-14|\n| 14|Name_14| 27| 66000|        HR|2022-08-21|\n| 15|Name_15| 33| 50000|   Finance|2020-02-13|\n| 16|Name_16| 44| 72000|        IT|2014-06-25|\n| 17|Name_17| 30| 80000|     Sales|2021-07-30|\n| 18|Name_18| 49| 93000|        HR|2016-05-09|\n| 19|Name_19| 36| 64000|   Finance|2019-12-02|\n| 20|Name_20| 31| 70000|        IT|2020-09-13|\n|  1| Name_1| 34| 45820|        HR|2018-03-15|\n|  2| Name_2| 45| 78000|        IT|2019-06-07|\n|  3| Name_3| 29| 62000|     Sales|2020-11-22|\n+---+-------+---+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#defining Unionall function\n",
    "union_df=df.unionAll(u_df)\n",
    "union_df.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47688399-7208-4520-9093-117c36b0ebac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88040b35-181d-48cc-8b50-1f86f2a76cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+----------+----------+------------+--------------+\n| ID|   Name| Age|Salary|Department|  JoinDate|Total_Salary|Average_Salary|\n+---+-------+----+------+----------+----------+------------+--------------+\n|  1| Name_1|  34| 45820|        HR|2018-03-15|        null|          null|\n|  2| Name_2|  45| 78000|        IT|2019-06-07|        null|          null|\n|  3| Name_3|  29| 62000|     Sales|2020-11-22|        null|          null|\n|  4| Name_4|  38| 54000|   Finance|2017-08-30|        null|          null|\n|  5| Name_5|  41| 95000|        IT|2015-02-10|        null|          null|\n|  6| Name_6|  55|120000|        HR|2013-04-12|        null|          null|\n|  7| Name_7|  26| 67000|     Sales|2021-01-06|        null|          null|\n|  8| Name_8|  50| 90000|   Finance|2016-07-19|        null|          null|\n|  9| Name_9|  60|110000|        IT|2014-10-11|        null|          null|\n| 10|Name_10|  35| 49000|        HR|2018-12-18|        null|          null|\n| 11|Name_11|  28| 53000|   Finance|2019-05-25|        null|          null|\n| 12|Name_12|  40| 75000|        IT|2017-03-03|        null|          null|\n| 13|Name_13|  52| 89000|     Sales|2013-09-14|        null|          null|\n| 14|Name_14|  27| 66000|        HR|2022-08-21|        null|          null|\n| 15|Name_15|  33| 50000|   Finance|2020-02-13|        null|          null|\n| 16|Name_16|  44| 72000|        IT|2014-06-25|        null|          null|\n| 17|Name_17|  30| 80000|     Sales|2021-07-30|        null|          null|\n| 18|Name_18|  49| 93000|        HR|2016-05-09|        null|          null|\n| 19|Name_19|  36| 64000|   Finance|2019-12-02|        null|          null|\n| 20|Name_20|  31| 70000|        IT|2020-09-13|        null|          null|\n|  1|   null|null|  null|        HR|      null|      370000|       61666.7|\n|  2|   null|null|  null|       ITI|      null|      564000|       70500.0|\n|  3|   null|null|  null|     Sales|      null|      335000|       67000.0|\n|  4|   null|null|  null|   Finance|      null|      365000|       61000.0|\n|  5|   null|null|  null|   Support|      null|       35000|       71600.0|\n+---+-------+----+------+----------+----------+------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#defining unionByName function\n",
    "u2_df=u_df.select( \"Name\", \"ID\", \"Age\", \"Salary\", \"Department\", \"JoinDate\")\n",
    "union_df=df.union(u2_df) # if both have same columns but the declarion is different then we can use it without parameter\n",
    "union_df=df.unionByName(s_df,True)# if both have differnt parameters then we can use the aprameter as true to make the union sof the missing colums values will be null for both\n",
    "union_df.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4e18cd6-ac9d-4986-b2b9-7112fa10bddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d08642df-532e-4183-956d-4368c41d4dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+----------+----------+\n| ID|   Name|Age|Salary|Department|  JoinDate|\n+---+-------+---+------+----------+----------+\n|  1| Name_1| 34| 45820|        HR|2018-03-15|\n|  2| Name_2| 45| 78000|        IT|2019-06-07|\n|  3| Name_3| 29| 62000|     Sales|2020-11-22|\n|  4| Name_4| 38| 54000|   Finance|2017-08-30|\n|  5| Name_5| 41| 95000|        IT|2015-02-10|\n|  6| Name_6| 55|     0|        HR|2013-04-12|\n|  7| Name_7| 26| 67000|     Sales|2021-01-06|\n|  8| Name_8| 50| 90000|   Finance|2016-07-19|\n|  9| Name_9| 60|     0|        IT|2014-10-11|\n| 10|Name_10| 35| 49000|        HR|2018-12-18|\n| 11|Name_11| 28| 53000|   Finance|2019-05-25|\n| 12|Name_12| 40| 75000|        IT|2017-03-03|\n| 13|Name_13| 52| 89000|     Sales|2013-09-14|\n| 14|Name_14| 27| 66000|        HR|2022-08-21|\n| 15|Name_15| 33| 50000|   Finance|2020-02-13|\n| 16|Name_16| 44| 72000|        IT|2014-06-25|\n| 17|Name_17| 30| 80000|     Sales|2021-07-30|\n| 18|Name_18| 49| 93000|        HR|2016-05-09|\n| 19|Name_19| 36| 64000|   Finance|2019-12-02|\n| 20|Name_20| 31| 70000|        IT|2020-09-13|\n+---+-------+---+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#defining fill or nafill function\n",
    "df1=df.na.fill(0,\"Salary\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ff4836-35e2-4f3c-b3b1-768cbede29ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 1\n"
     ]
    }
   ],
   "source": [
    "#defining fill or repartition function\n",
    "df1=df.repartition(\"Department\")\n",
    "num_partitions = df1.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "#partition by can be applied to write command to partition the data in folder"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Pyspark function and declaration",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
