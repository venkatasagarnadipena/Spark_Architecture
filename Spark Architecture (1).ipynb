{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d01a79-f11f-4bfd-afe3-e327f2e0f932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d63630-869f-498d-bc2b-20cc15e07264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"parquet\").load(\"/mnt/adls/target_tables/Dim/customers_table/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a390a30a-3730-4a5c-b4e1-e0bc63d0ecea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Understanding Job, Stage, and Task Creation in Spark Architecture\n",
    "\n",
    "### Example Question:\n",
    "We need to provide the count of customers whose phone number ends with '7' from the given DataFrame.\n",
    "\n",
    "### Breakdown of Spark Execution Flow\n",
    "\n",
    "### Explanation of the Flow:\n",
    "\n",
    "1. **User Query**:\n",
    "   - The user writes a query to filter customers whose phone number ends with '7' (e.g., df.filter(df.Phone.endswith(\"7\")).count()).\n",
    "\n",
    "\n",
    "2. **Driver Program**:\n",
    "   - The Driver Program parses the user query and constructs a Directed Acyclic Graph (DAG) that represents the stages of computation.\n",
    "   - The Driver passes this logical plan to the Catalyst Optimizer for optimization.\n",
    "   \n",
    "   1. ***Catalyst Optimizer***:\n",
    "      - The Catalyst Optimizer applies rule-based transformations to optimize the logical plan.\n",
    "      - It generates one or more physical plans based on the optimization and may apply cost-based optimization to choose the most efficient plan.\n",
    "   \n",
    "   - The driver program will create stages to apply the filter and perform the action to count the results.\n",
    "   - The driver program reaches out to the *Cluster Manager* for resource management.\n",
    "\n",
    "3. **Cluster Manager**:\n",
    "   - The cluster manager allocates resources (memory, CPU) and divides the data into partitions.\n",
    "   - The manager sends each partition to different worker nodes for parallel execution.\n",
    "\n",
    "4. **Worker Nodes (Executors)**:\n",
    "   - The worker nodes execute the task of filtering customers whose phone numbers end with '7'.\n",
    "   - Multiple worker nodes will process the partitions of the data in parallel.\n",
    "\n",
    "5. **Driver Program**:\n",
    "   - After task execution, the driver collects the filtered results from the worker nodes.\n",
    "   - The driver aggregates the results and returns the count of customers whose phone numbers end with '7' to the user.\n",
    "\n",
    "---\n",
    "## Flow Diagram\n",
    "\n",
    "```plaintext\n",
    "+-------------------+\n",
    "|   User Query      |         (Write query in notebook)\n",
    "|                   |         (Example: df.filter(col(\"Phone\").cast(StringType()).like(\"%7\")).count())\n",
    "+-------------------+ \n",
    "         |\n",
    "         v\n",
    "+-------------------+     \n",
    "|   Driver Program  |         (Creates DAG, plans execution)\n",
    "|                   |         (Filter rows, create actions)\n",
    "+-------------------+\n",
    "         |\n",
    "         v\n",
    "+-------------------+\n",
    "| Catalyst Optimizer|         (Optimizes the query plan)\n",
    "|                   |         (Logical Plan -> Optimized Logical Plan -> Physical Plan)\n",
    "+-------------------+\n",
    "         |\n",
    "         v\n",
    "+-------------------+     \n",
    "|  Cluster Manager  |         (Allocates resources, partitions data)\n",
    "+-------------------+  \n",
    "         |\n",
    "         v\n",
    "+-------------------+        +-------------------+\n",
    "| Worker Node 1     |<------>| Worker Node 2     |      (Parallel execution of tasks)\n",
    "| (Executor)        |        | (Executor)        |\n",
    "+-------------------+        +-------------------+ \n",
    "         |                        |\n",
    "         v                        v\n",
    "+-------------------+        +-------------------+\n",
    "| Task Execution    |        | Task Execution    |      (Filtering phone numbers ending with 7)\n",
    "| (on Data Partitions)|      | (on Data Partitions)|\n",
    "+-------------------+        +-------------------+\n",
    "         |\n",
    "         v\n",
    "+-------------------+\n",
    "| Driver Program    |         (Collect results and send them to user)\n",
    "| (Count result)    |         (Final count of customers)\n",
    "+-------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece1b2e9-1bca-4fab-99e0-3895585f4d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[28]: 5926"
     ]
    }
   ],
   "source": [
    "df=df.filter(col(\"Phone\").cast(StringType()).like(\"%7\"))\n",
    "# df=df.filter(col(\"Phone\").cast(IntegerType()).like(\"%7\"))\n",
    "df.count()\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4621bf9c-11b3-4c26-8936-7a9f55fe0d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Role of the Driver Node:\n",
    "\n",
    "The **Driver Node** is the main process that controls the execution of a Spark job. It does the following:\n",
    "\n",
    "1. **Receives the user's query**:\n",
    "   - The driver is responsible for accepting the Spark code (queries, transformations, and actions) written by the user, typically in the form of a Spark job.\n",
    "\n",
    "2. **Creates the DAG (Directed Acyclic Graph)**:\n",
    "   - The driver takes the code and converts it into a **DAG** that represents the sequence of operations (transformations) that need to be performed.\n",
    "\n",
    "3. **Plans and schedules the execution**:\n",
    "   - Once the DAG is created, the driver schedules tasks and breaks them down into smaller operations that can be executed across the Spark cluster.\n",
    "\n",
    "4. **Distributes tasks**:\n",
    "   - The driver sends the tasks (divided by partitions) to the **Worker Nodes (Executors)** for parallel execution.\n",
    "\n",
    "5. **Collects results**:\n",
    "   - After execution, the driver aggregates the results of the tasks and sends them back to the user.\n",
    "\n",
    "---\n",
    " - we have only 4 task in the diagram becasue for each executer node in our databricks cluster we have 4 cores to execute out task \n",
    " \n",
    "```plaintext\n",
    "+----------------------------------+\n",
    "|      User Query:                |\n",
    "|      df.filter(col(\"Phone\").cast(StringType()).like(\"%7\")).count() |\n",
    "+----------------------------------+\n",
    "                  |\n",
    "                  v\n",
    "         +------------------+ \n",
    "         |   Driver Node    |        (Creates DAG, plans the execution)\n",
    "         +------------------+\n",
    "                  |\n",
    "                  v\n",
    "    +-----------------------------+\n",
    "    |      Stage 1: Filter Rows    | (Filter rows where phone number ends with '7')\n",
    "    +-----------------------------+\n",
    "                  |\n",
    "                  v\n",
    "      +-------------------------+     \n",
    "      | Task 1 (Partition 1)     |        (Filter on partition 1)\n",
    "      +-------------------------+   \n",
    "                  |\n",
    "                  v\n",
    "      +-------------------------+     \n",
    "      | Task 2 (Partition 2)     |        (Filter on partition 2)\n",
    "      +-------------------------+ \n",
    "                  |\n",
    "                  v\n",
    "      +-------------------------+     \n",
    "      | Task 3 (Partition 3)     |        (Filter on partition 3)\n",
    "      +-------------------------+\n",
    "                  |\n",
    "                  v\n",
    "      +-------------------------+     \n",
    "      | Task 4 (Partition 4)     |        (Filter on partition 4)\n",
    "      +-------------------------+\n",
    "                  |\n",
    "                  v\n",
    "    +-----------------------------+\n",
    "    |   Action: count()           |        (Collect the results and count the filtered rows)\n",
    "    +-----------------------------+\n",
    "                  |\n",
    "                  v\n",
    "      +-------------------------+     \n",
    "      | Final Result (Count)     |        (Send final count back to user)\n",
    "      +-------------------------+ \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be12bea1-129a-458c-be86-f83bfb581f4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Steps in the DAG Creation:\n",
    "\n",
    "1. **The Driver reads the user query**:\n",
    "   - The driver receives the user's query (e.g., `df.filter(col(\"Phone\").cast(StringType()).like(\"%7\"))`).\n",
    "\n",
    "2. **The DAG for this query consists of one stage**:\n",
    "   - Since there’s no data shuffling involved in the `filter` operation, the DAG for this query is a **single stage**.\n",
    "\n",
    "3. **Stage 1**:\n",
    "   - This stage involves the `filter` transformation, which checks if the `Phone` column ends with '7'. \n",
    "   - This operation is a **narrow transformation**, meaning it does not require data shuffling between partitions. \n",
    "   - Hence, Spark executes this operation in a single stage, and it can be split across multiple tasks running on different partitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a087f03b-f8e2-485d-ac0d-e3e3b694b8ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Catalyst Optimizer Breakdown\n",
    "\n",
    "#### 1. **Analysis**\n",
    "- **Purpose**: The query is parsed and translated into a **Logical Plan**.\n",
    "- **Steps**:\n",
    "  - Spark begins by analyzing the user query.\n",
    "  - It parses the query to determine the structure of the operations and the underlying data.\n",
    "  - This results in a **Logical Plan**: a high-level, abstract representation of the transformations (such as filters, joins, etc.) that need to be applied to the data\n",
    "  \n",
    "  **Example**: \n",
    "  If the user writes `df.filter(col(\"Phone\").like(\"%7\")).count()`, the logical plan would describe the filtering and counting operations that need to be performed on the DataFrame `df`.\n",
    "\n",
    "#### 2. **Logical Optimization**\n",
    "- **Purpose**: Apply **rule-based optimizations** to the Logical Plan to improve query efficiency.\n",
    "- **Steps**:\n",
    "  - The Catalyst Optimizer applies various transformations to the logical plan to improve its efficiency.\n",
    "  - These transformations include:\n",
    "    - **Predicate Pushdown**: Moving filters closer to the data source to reduce the amount of data read. For example, if the filter is on a column (e.g., `Phone`), it tries to push this filter operation to the database or data source, which can handle it more efficiently.\n",
    "    - **Projection Pruning**: Removing unnecessary columns from the plan, ensuring that only the required columns are retrieved, thus saving memory and computation.\n",
    "    - **Constant Folding**: Simplifying expressions involving constants at compile time. If an expression like `1 + 2` is present, it will be optimized to `3` before execution.\n",
    "  \n",
    "  **Outcome**: This step reduces the cost of computation by simplifying the logical operations, improving the query plan’s overall efficiency.\n",
    "\n",
    "\n",
    "#### 3. **Physical Planning**\n",
    "- **Purpose**: Create a **Physical Plan** based on the optimized logical plan.\n",
    "- **Steps**:\n",
    "  - Once the logical plan has been optimized, the Catalyst Optimizer generates one or more **Physical Plans**.\n",
    "  - A **Physical Plan** specifies how the operations in the logical plan should be executed. It provides the detailed execution strategy, including how data will be read, processed, and written.\n",
    "  - Key components of physical planning include:\n",
    "    - **Join Strategy**: The physical plan will decide the most efficient join method to use (e.g., **broadcast join**, **shuffle join**, etc.) based on factors such as data size and distribution.\n",
    "    - **Data Partitioning**: Data will be partitioned and distributed across worker nodes. The physical plan specifies how data will be partitioned and processed in parallel to optimize performance.\n",
    "\n",
    "  **Outcome**: The physical plan provides a concrete, executable strategy for Spark to follow.\n",
    "\n",
    "\n",
    "#### 4. **Code Generation**\n",
    "- **Purpose**: Generate **executable code** based on the physical plan for execution on Spark.\n",
    "- **Steps**:\n",
    "  - The final step in the Catalyst Optimizer is to generate **executable code** for the physical plan.\n",
    "  - Spark uses **Tungsten** for code generation. This involves creating optimized JVM bytecode for each operation (like filtering, aggregating, etc.), ensuring that the operations are executed efficiently on the cluster.\n",
    "  - **Spark Jobs Code**: This code is sent to the cluster to be executed across the worker nodes.\n",
    "  \n",
    "  **Outcome**: Spark creates efficient, low-level code that is optimized for execution on the underlying hardware (such as CPUs, memory, and network).\n",
    "\n",
    "\n",
    "### Diagram Overview :\n",
    "\n",
    "```plaintext\n",
    "+-------------------+\n",
    "| Catalyst Optimizer|  <-- Optimizes the query using various transformations\n",
    "|                   |\n",
    "|   +-------------+ |\n",
    "|   |  Analysis   | |  <-- Parse query and create a logical plan\n",
    "|   | - Logical Plan| |\n",
    "|   +-------------+ |\n",
    "|         |         |\n",
    "|         v         |\n",
    "|   +-------------+ |\n",
    "|   | Logical     | |  <-- Apply rule-based optimizations\n",
    "|   | Optimization| |\n",
    "|   | - Predicate Pushdown| |\n",
    "|   | - Projection Pruning| |\n",
    "|   | - Constant Folding| |\n",
    "|   +-------------+ |\n",
    "|         |         |\n",
    "|         v         |\n",
    "|   +-------------+ |\n",
    "|   | Physical    | |  <-- Generate physical plans based on optimizations\n",
    "|   | Planning    | |\n",
    "|   | - Join Strategy | |\n",
    "|   | - Data Partitioning| |\n",
    "|   +-------------+ |\n",
    "|         |         |\n",
    "|         v         |\n",
    "|   +-------------+ |\n",
    "|   | Code        | |  <-- Generate executable code for execution\n",
    "|   | Generation  | |\n",
    "|   | - Spark Jobs Code| |\n",
    "|   +-------------+ |\n",
    "+-------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15849a8-dc08-4d44-b137-bbfefe6e84bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Filter cast('Phone as string) LIKE %7\n+- Filter cast(Phone#518 as string) LIKE %7\n   +- Filter cast(Phone#518 as string) LIKE %7\n      +- Relation [CustomerID#514,FirstName#515,LastName#516,Email#517,Phone#518,RegistrationDate#519,LastPurchaseDate#520,LastUpdated#521] parquet\n\n== Analyzed Logical Plan ==\nCustomerID: int, FirstName: string, LastName: string, Email: string, Phone: string, RegistrationDate: string, LastPurchaseDate: string, LastUpdated: string\nFilter cast(Phone#518 as string) LIKE %7\n+- Filter cast(Phone#518 as string) LIKE %7\n   +- Filter cast(Phone#518 as string) LIKE %7\n      +- Relation [CustomerID#514,FirstName#515,LastName#516,Email#517,Phone#518,RegistrationDate#519,LastPurchaseDate#520,LastUpdated#521] parquet\n\n== Optimized Logical Plan ==\nFilter (isnotnull(Phone#518) AND EndsWith(Phone#518, 7))\n+- Relation [CustomerID#514,FirstName#515,LastName#516,Email#517,Phone#518,RegistrationDate#519,LastPurchaseDate#520,LastUpdated#521] parquet\n\n== Physical Plan ==\n*(1) Filter (isnotnull(Phone#518) AND EndsWith(Phone#518, 7))\n+- *(1) ColumnarToRow\n   +- FileScan parquet [CustomerID#514,FirstName#515,LastName#516,Email#517,Phone#518,RegistrationDate#519,LastPurchaseDate#520,LastUpdated#521] Batched: true, DataFilters: [isnotnull(Phone#518), EndsWith(Phone#518, 7)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/adls/target_tables/Dim/customers_table], PartitionFilters: [], PushedFilters: [IsNotNull(Phone), StringEndsWith(Phone,7)], ReadSchema: struct<CustomerID:int,FirstName:string,LastName:string,Email:string,Phone:string,RegistrationDate...\n\n"
     ]
    }
   ],
   "source": [
    "# output of this command show the dag whihch will be generate in the back end based on our logical and physical plan whihc have the details of every steps in a catlist optimizer\n",
    "\n",
    "\n",
    "df.filter(col(\"Phone\").cast(StringType()).like(\"%7\")).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42369194-1e72-48eb-8c16-935273fbecb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Executor Memory Allocation Breakdown (8 GB Executor)\n",
    "\n",
    "#### 1. **Reserved Memory (300 MB)**:\n",
    "   - This portion is set aside for **internal use** by Spark or JVM overhead.\n",
    "   - **Not available** for user tasks or Spark execution/storage.\n",
    "   - **Example**: This memory might be used for managing Spark's internal state, JVM overhead, or system tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Spark Memory (50% of total memory)**:\n",
    "   - The total memory allocated to each executor for tasks is **8 GB**.\n",
    "   - The Spark memory is **divided** into two parts:\n",
    "   \n",
    "   - ##### **Execution Memory (60% of 4 GB = 2.4 GB)**:\n",
    "     - **Purpose**: Used for computation tasks like shuffling, sorting, filtering, and joining data.\n",
    "     - **Example Task**: For the task `df.filter(col(\"Phone\").like(\"%7\")).count()`, Spark:\n",
    "       - Filters the rows in the DataFrame where the \"Phone\" column contains the digit \"7\".\n",
    "       - **Memory Use**: Execution memory is used to store temporary results while performing this filtering task.\n",
    "       - **Memory Allocated**: **4.8 GB** (60% of 8 GB) is used for this task's execution.\n",
    "   \n",
    "   - ##### **Storage Memory (40% of 4 GB = 1.6  GB)**:\n",
    "     - **Purpose**: Used for **caching** or **persisting** data (e.g., RDDs, DataFrames) for future stages or reuse.\n",
    "     - **Example Task**: If you cache the DataFrame `df`, it will occupy the storage memory:\n",
    "       - `df.cache()` stores the data in memory for faster access across stages.\n",
    "       - **Memory Use**: Storage memory is allocated to hold the DataFrame or filtered data if cached.\n",
    "       - **Memory Allocated**: **1.6 GB** (40% of 4 GB) is available for storing this cached data.\n",
    "\n",
    "   - ##### **Slots**:\n",
    "      - The **Spark memory** is divided into 4 (this is only for databricks clusters) **slots** (`slot1`, `slot2`, `slot3`, `slot4`).\n",
    "      - Each **slot** is used for parallel task execution.\n",
    "      - **Example Task**: For the `df.filter(col(\"Phone\").like(\"%7\")).count()` operation, the task might be split into multiple partitions, and each partition is processed in parallel using a separate slot.\n",
    "      - If there are 4 partitions in the DataFrame, Spark could assign each partition to a separate slot, allowing the task to run **concurrently**.\n",
    "\n",
    "- #### **Note**\n",
    "    - Execution Memory or Storage Memory can be increased or decresed based on the requriments\n",
    "---\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **User Memory (40% of total memory = 3.2 GB)**:\n",
    "   - Represents the **memory allocated** for the user’s data and tasks.\n",
    "   - The user memory is used to store **intermediate data** and **results** for the task.\n",
    "   - **Example Task**: For `df.filter(col(\"Phone\").like(\"%7\")).count()`, the filtered data (matching \"Phone\" values with \"7\") will be temporarily stored in user memory before counting the rows.\n",
    "   - **Memory Allocated**: **3.2 GB** (40% of 8 GB) is available for the user’s data and task results.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concept:\n",
    "- ### **Unified Memory Manager**: \n",
    "  - Allows Spark to **dynamically allocate unused memory** between **execution** and **storage memory**, depending on which one is underutilized.\n",
    "  - **For example**: If execution memory is not fully used, Spark can allocate some of it to storage memory (for caching), improving resource utilization.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example Scenario (8 GB Executor):\n",
    "\n",
    "Let’s say we have **8 GB of memory allocated** to each executor. Here’s how the memory is allocated based on the task `df.filter(col(\"Phone\").like(\"%7\")).count()`:\n",
    "\n",
    "1. **Reserved Memory (300 MB)**:\n",
    "   - **300 MB** is reserved for Spark’s internal overhead. This memory will not be used for user tasks like filtering.\n",
    "\n",
    "2. **Executor Memory (50% = 4 GB)**:\n",
    "   - **Execution Memory (60% = 2.4 GB)**: This memory will handle the computation for filtering the DataFrame based on the \"Phone\" column containing \"7\".\n",
    "   - **Storage Memory (40% = 1.6 GB)**: If the filtered DataFrame is cached for future stages or reuse, it will be stored in the 3.2 GB of storage memory.\n",
    "\n",
    "3. **Slots**:\n",
    "   - The executor memory (4 GB) is divided into **4 slots** for parallel execution.\n",
    "   - **Each slot** gets **1 GB** of memory, and each slot processes a partition of the DataFrame in parallel.\n",
    "     - For example, if the DataFrame `df` has 4 partitions, Spark may process each partition in parallel across 4 slots.\n",
    "\n",
    "4. **User Memory (40% = 3.2 GB)**:\n",
    "   - The task `df.filter(col(\"Phone\").like(\"%7\")).count()` will utilize **3.2 GB** of user memory to store intermediate results and final results.\n",
    "   - After the filtering, the count of matching rows is stored in the user memory.\n",
    "\n",
    "---\n",
    "\n",
    "#### Memory Management with Unified Memory Manager:\n",
    "- The **Unified Memory Manager** in Spark ensures that unused memory from **Execution Memory** and **Storage Memory** can be dynamically shared.\n",
    "- If the **Execution Memory** is underutilized while filtering, it can be reallocated for **Storage Memory**, allowing more data to be cached.\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Summary:\n",
    "- **Reserved Memory**: 300 MB for Spark's internal overhead.\n",
    "- **Executor Memory** (4 GB): Divided into **Execution Memory** (2.4 GB) for processing tasks and **Storage Memory** (1.6 GB) for caching.\n",
    "- **Slots**: The executor can process tasks in parallel using 4 slots, each with 1 GB of memory.\n",
    "- **User Memory**: 3.2 GB is used for user data during task execution.\n",
    "\n",
    "---\n",
    "\n",
    "### Diagram Overview :  \n",
    "\n",
    "\n",
    "```plaintext\n",
    "\n",
    "+-------------------------------+\n",
    "|   reserved memeory 300mb      |\n",
    "|-------------------------------|<--+      +--------------------------------+\n",
    "|+-----------------------------+|   |      |        |       |       |       |\n",
    "||                             ||   |      | slot1  | slot2 | slot3 | slot4 | \n",
    "||   storage memeory (50%)     ||   |      |        |       |       |       | \n",
    "||                             ||   |      +--------------------------------+\n",
    "|+-----------------------------+|   |------->(spark memory 60%)it has slots to execute our code      \n",
    "||                             ||   |\n",
    "||   Executer memeory (50%)    ||   |\n",
    "||                             ||   |\n",
    "|+-----------------------------+|   |\n",
    "|-------------------------------|<--+\n",
    "|                               |\n",
    "|   user memeory 40%            |\n",
    "|                               |\n",
    "|                               |\n",
    "+-------------------------------+\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Spark Architecture",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
