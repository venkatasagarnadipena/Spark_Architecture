{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "069ceac0-17c5-4a93-a0c4-63533840d2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc7cf84-c9da-4f46-802a-94ef0d8b4c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/adls/target_tables/Dim/categories_table/</td><td>categories_table/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/adls/target_tables/Dim/customers_table/</td><td>customers_table/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/adls/target_tables/Dim/material_table/</td><td>material_table/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/adls/target_tables/Dim/products_table/</td><td>products_table/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/adls/target_tables/Dim/roles_table/</td><td>roles_table/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/adls/target_tables/Dim/suppliers_table/</td><td>suppliers_table/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/adls/target_tables/Dim/users_table/</td><td>users_table/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/adls/target_tables/Dim/warehouses_table/</td><td>warehouses_table/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/adls/target_tables/Dim/categories_table/",
         "categories_table/",
         0,
         0
        ],
        [
         "dbfs:/mnt/adls/target_tables/Dim/customers_table/",
         "customers_table/",
         0,
         0
        ],
        [
         "dbfs:/mnt/adls/target_tables/Dim/material_table/",
         "material_table/",
         0,
         0
        ],
        [
         "dbfs:/mnt/adls/target_tables/Dim/products_table/",
         "products_table/",
         0,
         0
        ],
        [
         "dbfs:/mnt/adls/target_tables/Dim/roles_table/",
         "roles_table/",
         0,
         0
        ],
        [
         "dbfs:/mnt/adls/target_tables/Dim/suppliers_table/",
         "suppliers_table/",
         0,
         0
        ],
        [
         "dbfs:/mnt/adls/target_tables/Dim/users_table/",
         "users_table/",
         0,
         0
        ],
        [
         "dbfs:/mnt/adls/target_tables/Dim/warehouses_table/",
         "warehouses_table/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /mnt/adls/target_tables/Dim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9e44ddcf-3b68-427c-817a-3c1fc39c9a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Narrow Transformations in Spark\n",
    "\n",
    "A **narrow transformation** in Spark refers to operations on **RDDs (Resilient Distributed Datasets)** that only involve data from a **single partition**. These transformations do **not require shuffling** of data between partitions, making them more efficient than wide transformations, which involve data exchange across partitions.\n",
    "\n",
    "## Key Characteristics:\n",
    "- **Single Partition**: Narrow transformations process data within each partition independently.\n",
    "- **No Shuffling**: Data does not need to be redistributed or shuffled across nodes in the cluster.\n",
    "- **Partitioning Preserved**: The partitioning scheme of the input RDD is generally maintained after the transformation.\n",
    "\n",
    "## Common Narrow Transformations:\n",
    "1. **map()**: \n",
    "   - Applies a function to each element in the RDD and returns a new RDD with the transformed elements.\n",
    "   \n",
    "2. **filter()**: \n",
    "   - Filters out elements based on a condition, returning only the elements that satisfy the condition.\n",
    "   \n",
    "3. **flatMap()**: \n",
    "   - Similar to `map()`, but can return multiple values for each input element, which are then flattened into a single RDD.\n",
    "   \n",
    "4. **mapPartitions()**: \n",
    "   - Applies a function to entire partitions (not individual elements), but still operates within the same partition.\n",
    "   \n",
    "5. **union()**: \n",
    "   - Combines two RDDs into one without shuffling data, preserving the partitioning of both RDDs.\n",
    "   \n",
    "6. **sample()**: \n",
    "   - Randomly samples data from the RDD without requiring a shuffle.\n",
    "   \n",
    "7. **zip()**: \n",
    "   - Combines two RDDs element-wise into pairs without shuffling.\n",
    "\n",
    "## Why Narrow Transformations are Efficient:\n",
    "- **Faster Execution**: Since no shuffling is required, narrow transformations are generally faster and require less network I/O and disk usage.\n",
    "- **Lower Overhead**: The lack of data movement across partitions reduces computational and network overhead.\n",
    "\n",
    "## Example:\n",
    "```python\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "result = rdd.map(lambda x: x * 2)  # map is a narrow transformation\n",
    "print(result.collect())  # Output: [2, 4, 6, 8, 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "508c577a-7f34-45a5-a1e2-6b1fdd853402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| **Narrow Transformation** | **Description**                                                       |\n",
    "|---------------------------|-----------------------------------------------------------------------|\n",
    "| **map()**                 | Applies a function to each element in the RDD.                      |\n",
    "| **filter()**              | Returns a new RDD containing elements that meet a specified condition. |\n",
    "| **flatMap()**             | Similar to `map`, but allows returning multiple elements for each input element. |\n",
    "| **union()**               | Combines two RDDs into one, containing all elements from both.      |\n",
    "| **sample()**              | Returns a random sample of the RDD.                                 |\n",
    "| **zip()**                 | Combines two RDDs by pairing corresponding elements together.        |\n",
    "| **coalesce()**            | Reduces the number of partitions in the RDD without performing a shuffle. |\n",
    "| **mapPartitions()**       | Applies a function to each partition of the RDD, allowing more efficient operations. |\n",
    "| **distinct()**            | Returns a new RDD containing unique elements (generally narrow but can involve wider behavior depending on context). |\n",
    "| **sortBy()**              | Sorts the elements of the RDD based on a specified key or condition. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "087dd434-2c64-436c-9f3c-2db586777b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Narrow Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4be86c19-33d0-44d4-ab36-1016378b7a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Narrow Transformations in Spark DataFrames\n",
    "\n",
    "A **narrow transformation** in Spark for **DataFrames** refers to operations that only involve a **single partition** and do not require shuffling of data between partitions. These transformations are more efficient than **wide transformations**, which involve data exchange across partitions.\n",
    "\n",
    "## Key Characteristics:\n",
    "\n",
    "- **Single Partition**: Narrow transformations work within a single partition of the DataFrame without involving other partitions.\n",
    "\n",
    "- **No Shuffling**: Data does not need to be moved or shuffled across partitions, which reduces network I/O and speeds up execution.\n",
    "\n",
    "- **Partitioning Preserved**: The partitioning of the input DataFrame is usually maintained after the transformation, unless explicitly altered.\n",
    "\n",
    "## Common Narrow Transformations:\n",
    "\n",
    "1. **select()**:\n",
    "\n",
    "   - Selects a subset of columns from a DataFrame, creating a new DataFrame with only the specified columns.\n",
    "\n",
    "2. **filter()**:\n",
    "\n",
    "   - Filters rows based on a condition, returning only those that satisfy the given filter expression.\n",
    "\n",
    "3. **map()** (using `rdd` API):\n",
    "\n",
    "   - Similar to `map()` on RDDs, it allows applying a function to each row of the DataFrame and can return a transformed DataFrame or RDD.\n",
    "\n",
    "4. **withColumn()**:\n",
    "\n",
    "   - Adds a new column to the DataFrame, applying a transformation to an existing column or expression.\n",
    "\n",
    "5. **drop()**:\n",
    "\n",
    "   - Removes one or more columns from the DataFrame, creating a new DataFrame without the specified columns.\n",
    "\n",
    "6. **distinct()**:\n",
    "\n",
    "   - Returns a new DataFrame with only the distinct rows, removing duplicates without shuffling data.\n",
    "\n",
    "7. **limit()**:\n",
    "\n",
    "   - Returns a new DataFrame with only the first `n` rows of the original DataFrame, without requiring a shuffle.\n",
    "\n",
    "8. **alias()**:\n",
    "\n",
    "   - Renames a column with a new alias in a DataFrame.\n",
    "\n",
    "9. **join()** (when applied within a single partition):\n",
    "\n",
    "   - Performs an inner or outer join within the same partition, without needing to shuffle data between partitions.\n",
    "\n",
    "## Why Narrow Transformations are Efficient:\n",
    "\n",
    "- **Faster Execution**: Since no shuffling is required, narrow transformations are faster and consume less memory and network I/O.\n",
    "\n",
    "- **Lower Overhead**: The data is processed locally within each partition, reducing computational overhead and avoiding unnecessary data movement.\n",
    "\n",
    "## Example:\n",
    "\n",
    "```python\n",
    "# Example with a DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NarrowTransformations\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\"), (3, \"Cathy\")], [\"id\", \"name\"])\n",
    "\n",
    "# Narrow transformation: using 'select' to pick columns\n",
    "result = df.select(\"name\")\n",
    "\n",
    "# Show result\n",
    "result.show()\n",
    "# Output:\n",
    "# +-----+\n",
    "# | name|\n",
    "# +-----+\n",
    "# |Alice|\n",
    "# |  Bob|\n",
    "# |Cathy|\n",
    "# +-----+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b52305-ae4f-4e65-8b58-55d9ed43fe21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Common Narrow Transformations in DataFrames:\n",
    "\n",
    "| **Narrow Transformation**   | **Description**                                                        |\n",
    "|-----------------------------|------------------------------------------------------------------------|\n",
    "| **select()**                 | Selects a subset of columns from the DataFrame.                        |\n",
    "| **filter()**                 | Filters rows based on a condition.                                    |\n",
    "| **withColumn()**             | Adds or updates a column in the DataFrame with a specified transformation. |\n",
    "| **drop()**                   | Removes one or more columns from the DataFrame.                        |\n",
    "| **distinct()**               | Returns a DataFrame with distinct rows, removing duplicates.           |\n",
    "| **limit()**                  | Limits the number of rows returned by the DataFrame to `n`.            |\n",
    "| **alias()**                  | Renames a column with a new alias.                                     |\n",
    "| **selectExpr()**             | Allows selecting columns and performing transformations via SQL expressions. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a657d1e9-7176-4950-a504-6448395cd81c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22784688-31cc-4ce5-a8be-40fed4425554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------------+----------------+-------------------+\n|CategoryID| CategoryName|ParentCategoryID|NumberOfProducts|        LastUpdated|\n+----------+-------------+----------------+----------------+-------------------+\n|      7250|Category_7250|            5132|             414|2024-11-25 08:43:41|\n|      7251|Category_7251|            1793|              79|2024-11-02 08:43:41|\n|      7252|Category_7252|            5501|             414|2024-11-07 08:43:41|\n|      7253|Category_7253|            2682|              21|2024-11-10 08:43:41|\n|      7254|Category_7254|            2434|               7|2024-11-15 08:43:41|\n|      7255|Category_7255|            null|              70|2024-11-29 08:43:41|\n|      7256|Category_7256|            7215|             365|2024-11-10 08:43:41|\n|      7257|Category_7257|            null|             195|2024-11-05 08:43:41|\n|      7258|Category_7258|            null|             170|2024-11-08 08:43:41|\n|      7259|Category_7259|            1052|             152|2024-11-04 08:43:41|\n|      7260|Category_7260|            null|              14|2024-11-09 08:43:41|\n|      7261|Category_7261|            2506|             457|2024-11-05 08:43:41|\n|      7262|Category_7262|            null|             176|2024-11-25 08:43:41|\n|      7263|Category_7263|            null|              85|2024-11-30 08:43:41|\n|      7264|Category_7264|            7981|              56|2024-11-21 08:43:41|\n|      7265|Category_7265|            1967|             168|2024-11-19 08:43:41|\n|      7266|Category_7266|            4891|             428|2024-11-11 08:43:41|\n|      7267|Category_7267|            6606|             302|2024-11-27 08:43:41|\n|      7268|Category_7268|            2907|             102|2024-11-09 08:43:41|\n|      7269|Category_7269|            null|             351|2024-11-23 08:43:41|\n+----------+-------------+----------------+----------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##Select condition \n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df=df.select('CategoryID','CategoryName','ParentCategoryID','NumberOfProducts','LastUpdated')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6501c203-d941-4961-ab81-dbc06580dd44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When executing the command with `select()` in Spark, you might observe a discrepancy between the **logical row count** (e.g., 7000 records) and the **read rows** value in the DAG metrics (e.g., 875 rows). This difference can occur due to a few reasons related to how Spark optimizes data loading and processing.\n",
    "\n",
    "#### 1. **Column Pruning**:\n",
    "- **What is Column Pruning?**\n",
    "  - When you use the `select()` operation, Spark only loads the specified columns from the dataset. This is known as **column pruning**.\n",
    "  - **Impact on Read Rows**: Column pruning reduces the amount of data loaded into memory because Spark only reads the necessary columns. However, it **does not reduce the number of rows read** from disk. Spark might still need to scan the entire dataset to read the specified columns, especially if the data is not partitioned efficiently.\n",
    "  - As a result, the \"read rows\" metric could still show a higher number (e.g., 875) because Spark is reading the full set of rows from the Parquet file, even though it is only loading a subset of the columns into memory.\n",
    "\n",
    "#### 2. **Efficient Data Loading**:\n",
    "- **What is Efficient Data Loading?**\n",
    "  - Spark employs several techniques to reduce the amount of data it reads, such as **partition pruning** and **predicate pushdown**.\n",
    "    - **Partition Pruning**: If your dataset is partitioned (e.g., by `CategoryID` or `LastUpdated`), Spark may skip reading irrelevant partitions.\n",
    "    - **Predicate Pushdown**: If a `WHERE` clause or filter is applied, Spark can push down these filters to the storage layer (e.g., Parquet), which reduces the number of rows loaded.\n",
    "  - **Impact on Read Rows**: These optimizations help Spark read **only the necessary data**. However, even when these techniques are applied, the DAG metrics might still report a higher number of rows being read (875 in this case) because Spark could be reading additional rows or partitions to apply its optimizations, depending on how the data is stored and partitioned.\n",
    "\n",
    "#### Summary:\n",
    "- The difference between the logical row count (7000 rows) and the \"read rows\" metric (875 rows) can be attributed to Spark's internal optimizations, such as column pruning and efficient data loading techniques.\n",
    "- While Spark might only load specific columns into memory, it could still be reading a larger number of rows from disk as part of its optimizations or based on how the data is structured (e.g., partitioning, filtering).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b1a357f-f950-4c3b-b305-a35cdfeb36d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+-----------+----------------+-----------+-----------+\n|CategoryID|CategoryName|ParentCategoryID|Description|NumberOfProducts|CreatedDate|LastUpdated|\n+----------+------------+----------------+-----------+----------------+-----------+-----------+\n+----------+------------+----------------+-----------+----------------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "##Filter condition \n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df=df.filter(df.ParentCategoryID==9166)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f49ed9-3a1b-4bab-a3ec-8e949e4f8b40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observation:\n",
    "\n",
    "A total of 4 jobs are generated in this step:\n",
    "1. One for reading the data\n",
    "2. One for filtering the data\n",
    "3. One for showing the output\n",
    "4. And one for some additional processing (it got created in run runtime or specific output)\n",
    "\n",
    "However, from the output, we can see that 4 jobs were created, but in the UI, we can only see 3 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c377d20-4aaf-4d48-bb71-b035cf243965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------------+--------------------+----------------+-------------------+-------------------+-----------------+\n|CategoryID| CategoryName|ParentCategoryID|         Description|NumberOfProducts|        CreatedDate|        LastUpdated|NumberOfProducts2|\n+----------+-------------+----------------+--------------------+----------------+-------------------+-------------------+-----------------+\n|      7250|Category_7250|            5132|This is a descrip...|             414|2024-01-01 08:43:41|2024-11-25 08:43:41|              828|\n|      7251|Category_7251|            1793|This is a descrip...|              79|2024-08-26 08:43:41|2024-11-02 08:43:41|              158|\n|      7252|Category_7252|            5501|This is a descrip...|             414|2024-11-21 08:43:41|2024-11-07 08:43:41|              828|\n|      7253|Category_7253|            2682|This is a descrip...|              21|2024-03-26 08:43:41|2024-11-10 08:43:41|               42|\n|      7254|Category_7254|            2434|This is a descrip...|               7|2024-09-30 08:43:41|2024-11-15 08:43:41|               14|\n|      7255|Category_7255|            null|This is a descrip...|              70|2024-04-26 08:43:41|2024-11-29 08:43:41|              140|\n|      7256|Category_7256|            7215|This is a descrip...|             365|2024-06-12 08:43:41|2024-11-10 08:43:41|              730|\n|      7257|Category_7257|            null|This is a descrip...|             195|2024-04-05 08:43:41|2024-11-05 08:43:41|              390|\n|      7258|Category_7258|            null|This is a descrip...|             170|2024-08-10 08:43:41|2024-11-08 08:43:41|              340|\n|      7259|Category_7259|            1052|This is a descrip...|             152|2024-04-21 08:43:41|2024-11-04 08:43:41|              304|\n|      7260|Category_7260|            null|This is a descrip...|              14|2024-04-06 08:43:41|2024-11-09 08:43:41|               28|\n|      7261|Category_7261|            2506|This is a descrip...|             457|2024-08-10 08:43:41|2024-11-05 08:43:41|              914|\n|      7262|Category_7262|            null|This is a descrip...|             176|2024-08-02 08:43:41|2024-11-25 08:43:41|              352|\n|      7263|Category_7263|            null|This is a descrip...|              85|2024-03-12 08:43:41|2024-11-30 08:43:41|              170|\n|      7264|Category_7264|            7981|This is a descrip...|              56|2024-06-11 08:43:41|2024-11-21 08:43:41|              112|\n|      7265|Category_7265|            1967|This is a descrip...|             168|2024-04-13 08:43:41|2024-11-19 08:43:41|              336|\n|      7266|Category_7266|            4891|This is a descrip...|             428|2024-09-25 08:43:41|2024-11-11 08:43:41|              856|\n|      7267|Category_7267|            6606|This is a descrip...|             302|2024-10-13 08:43:41|2024-11-27 08:43:41|              604|\n|      7268|Category_7268|            2907|This is a descrip...|             102|2024-09-11 08:43:41|2024-11-09 08:43:41|              204|\n|      7269|Category_7269|            null|This is a descrip...|             351|2024-01-10 08:43:41|2024-11-23 08:43:41|              702|\n+----------+-------------+----------------+--------------------+----------------+-------------------+-------------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##withColumn condition \n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df=df.withColumn(\"NumberOfProducts2\",df.NumberOfProducts*2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e0f7b9-4b01-44a7-bc76-1ef402df0f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(1) Project [CategoryID#262, CategoryName#263, ParentCategoryID#264, Description#265, NumberOfProducts#266, CreatedDate#267, LastUpdated#268, (NumberOfProducts#266 * 2) AS NumberOfProducts2#276]\n+- *(1) ColumnarToRow\n   +- FileScan parquet [CategoryID#262,CategoryName#263,ParentCategoryID#264,Description#265,NumberOfProducts#266,CreatedDate#267,LastUpdated#268] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/adls/target_tables/Dim/categories_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<CategoryID:int,CategoryName:string,ParentCategoryID:int,Description:string,NumberOfProduct...\n\n\n"
     ]
    }
   ],
   "source": [
    "# execution plan\n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df=df.withColumn(\"NumberOfProducts2\",df.NumberOfProducts*2).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201d5517-5448-4213-9242-1debff992ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observation: \n",
    "As withColumn condition only addes the new column based on the logic so we have only Job in our case same as select condition. the actual column creatiuon happed in the project part of the execution plan only  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7b93c0a-9f2d-41e2-8969-b2e94791977b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------------+--------------------+-------------------+-------------------+\n|CategoryID| CategoryName|ParentCategoryID|         Description|        CreatedDate|        LastUpdated|\n+----------+-------------+----------------+--------------------+-------------------+-------------------+\n|      7250|Category_7250|            5132|This is a descrip...|2024-01-01 08:43:41|2024-11-25 08:43:41|\n|      7251|Category_7251|            1793|This is a descrip...|2024-08-26 08:43:41|2024-11-02 08:43:41|\n|      7252|Category_7252|            5501|This is a descrip...|2024-11-21 08:43:41|2024-11-07 08:43:41|\n|      7253|Category_7253|            2682|This is a descrip...|2024-03-26 08:43:41|2024-11-10 08:43:41|\n|      7254|Category_7254|            2434|This is a descrip...|2024-09-30 08:43:41|2024-11-15 08:43:41|\n|      7255|Category_7255|            null|This is a descrip...|2024-04-26 08:43:41|2024-11-29 08:43:41|\n|      7256|Category_7256|            7215|This is a descrip...|2024-06-12 08:43:41|2024-11-10 08:43:41|\n|      7257|Category_7257|            null|This is a descrip...|2024-04-05 08:43:41|2024-11-05 08:43:41|\n|      7258|Category_7258|            null|This is a descrip...|2024-08-10 08:43:41|2024-11-08 08:43:41|\n|      7259|Category_7259|            1052|This is a descrip...|2024-04-21 08:43:41|2024-11-04 08:43:41|\n|      7260|Category_7260|            null|This is a descrip...|2024-04-06 08:43:41|2024-11-09 08:43:41|\n|      7261|Category_7261|            2506|This is a descrip...|2024-08-10 08:43:41|2024-11-05 08:43:41|\n|      7262|Category_7262|            null|This is a descrip...|2024-08-02 08:43:41|2024-11-25 08:43:41|\n|      7263|Category_7263|            null|This is a descrip...|2024-03-12 08:43:41|2024-11-30 08:43:41|\n|      7264|Category_7264|            7981|This is a descrip...|2024-06-11 08:43:41|2024-11-21 08:43:41|\n|      7265|Category_7265|            1967|This is a descrip...|2024-04-13 08:43:41|2024-11-19 08:43:41|\n|      7266|Category_7266|            4891|This is a descrip...|2024-09-25 08:43:41|2024-11-11 08:43:41|\n|      7267|Category_7267|            6606|This is a descrip...|2024-10-13 08:43:41|2024-11-27 08:43:41|\n|      7268|Category_7268|            2907|This is a descrip...|2024-09-11 08:43:41|2024-11-09 08:43:41|\n|      7269|Category_7269|            null|This is a descrip...|2024-01-10 08:43:41|2024-11-23 08:43:41|\n+----------+-------------+----------------+--------------------+-------------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##Drop condition \n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df=df.drop(\"NumberOfProducts\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33033593-cffd-406e-99b7-0c6097b38c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observation: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dffa47f7-7025-4ef6-9102-7e112a46be4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions before distinct: 8\nNumber of partitions after distinct: 1\n+----------+----------------+----------------+\n|CategoryID|ParentCategoryID|NumberOfProducts|\n+----------+----------------+----------------+\n|      7344|            3981|             250|\n|      7407|            1159|              35|\n|      7534|            null|              96|\n|      7541|            4732|             186|\n|      7582|            2113|              86|\n|      7726|            null|             434|\n|      7757|            null|             348|\n|      7774|            null|             221|\n|      7903|            null|             321|\n|      7957|            3313|              31|\n|      7279|            7855|             237|\n|      7680|            9557|             330|\n|      7721|            null|             297|\n|      8016|            null|             363|\n|      8089|            null|              46|\n|      7302|            1409|             306|\n|      7333|            9648|             488|\n|      7480|            1789|              54|\n|      7572|            3110|             307|\n|      7730|            null|             395|\n+----------+----------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##Distinct \n",
    "#ADQ Enabled\n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "print(\"Number of partitions before distinct:\", df.rdd.getNumPartitions())\n",
    "df=df.select(df.CategoryID, df.ParentCategoryID, df.NumberOfProducts).distinct()\n",
    "print(\"Number of partitions after distinct:\", df.rdd.getNumPartitions())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b810829-4391-4dd5-8ff8-11fa118d3274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: '200'"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c821375-47a1-4e10-baab-67a977243ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions before distinct: 8\nNumber of partitions after distinct: 200\n+----------+----------------+----------------+\n|CategoryID|ParentCategoryID|NumberOfProducts|\n+----------+----------------+----------------+\n|      7344|            3981|             250|\n|      7407|            1159|              35|\n|      7534|            null|              96|\n|      7541|            4732|             186|\n|      7582|            2113|              86|\n|      7726|            null|             434|\n|      7757|            null|             348|\n|      7774|            null|             221|\n|      7903|            null|             321|\n|      7957|            3313|              31|\n|      4934|            null|              18|\n|      5061|            1939|              29|\n|      5100|            null|             461|\n|      5147|            null|               4|\n|      5180|            8783|             314|\n|      5287|            8038|              82|\n|      5516|            6851|              81|\n|      5766|            null|             365|\n|      5767|            null|              55|\n|      5798|            null|             167|\n+----------+----------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##Distinct \n",
    "#ADQ Disbaled\n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "print(\"Number of partitions before distinct:\", df.rdd.getNumPartitions())\n",
    "df=df.select(df.CategoryID, df.ParentCategoryID, df.NumberOfProducts).distinct()\n",
    "print(\"Number of partitions after distinct:\", df.rdd.getNumPartitions())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb03f0dc-5e1d-46b9-8e18-a56bc4477619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observation:\n",
    "\n",
    "#### Run with AQE Enabled (Adaptive Query Execution):\n",
    "When we run the code with AQE enabled, a shuffle operation occurs due to the `distinct()` function. The reasons for this behavior are as follows:\n",
    "\n",
    "1. **Distributed Data:**\n",
    "   - When your data is large and distributed across many partitions, Spark needs to ensure that the `distinct()` operation is applied across the entire dataset.\n",
    "   - This requires shuffling the data, especially for operations that involve grouping, like `distinct()`. Without this shuffle, Spark cannot guarantee that each partition will have unique rows.\n",
    "\n",
    "2. **HashAggregate:**\n",
    "   - The `distinct()` operation in Spark is essentially a `groupBy` operation.\n",
    "   - For large datasets, Spark may need to perform a shuffle to aggregate the data correctly across partitions. This shuffle is necessary because:\n",
    "     - Each partition holds a portion of the data.\n",
    "     - Spark needs to combine these portions to eliminate duplicates globally, across all partitions, not just within a single partition.\n",
    "\n",
    "3. **Adaptive Query Execution (AQE):**\n",
    "   - AQE optimizes queries at runtime based on the actual data distribution.\n",
    "   - In this case, Spark may choose to perform an exchange and shuffle if it determines that doing so will improve performance. For example, this could occur if partitions are unevenly distributed or if the data is skewed.\n",
    "   - Even though the partition count may reduce (from 8 to 1, as seen in the output), AQE might still perform a shuffle to optimize the query.\n",
    "\n",
    "#### Run with AQE Disabled:\n",
    "When AQE is disabled, no shuffle operation occurs in the code, and the partition count increases. The reasons are as follows:\n",
    "\n",
    "- **No Dynamic Optimizations:**\n",
    "  - With AQE disabled, Spark does not adjust the partition count dynamically based on the data distribution. Instead, it uses the static partitioning strategy.\n",
    "  - By default, Spark sets the number of shuffle partitions to **200**, which leads to an increased number of partitions, as reflected in the output.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- **With AQE enabled**, Spark performs a shuffle for operations like `distinct()` to ensure global uniqueness of the data. AQE also dynamically adjusts the number of partitions during execution to optimize performance (e.g., reducing partitions from 8 to 1 in your case).\n",
    "- **With AQE disabled**, Spark uses static partitioning, and the default shuffle partition count (typically **200**) is applied, leading to an increased number of partitions, regardless of the data size.\n",
    "- Because of which we got 2 jobs in Spark UI for enabled as it become wide transformation and only 1 job for disabled as it be narrow transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa33c6d-0fbb-4784-b1db-5f6261f2ae33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------------+--------------------+----------------+-------------------+-------------------+\n|CategoryID| CategoryName|ParentCategoryID|         Description|NumberOfProducts|        CreatedDate|        LastUpdated|\n+----------+-------------+----------------+--------------------+----------------+-------------------+-------------------+\n|      7250|Category_7250|            5132|This is a descrip...|             414|2024-01-01 08:43:41|2024-11-25 08:43:41|\n|      7251|Category_7251|            1793|This is a descrip...|              79|2024-08-26 08:43:41|2024-11-02 08:43:41|\n|      7252|Category_7252|            5501|This is a descrip...|             414|2024-11-21 08:43:41|2024-11-07 08:43:41|\n|      7253|Category_7253|            2682|This is a descrip...|              21|2024-03-26 08:43:41|2024-11-10 08:43:41|\n|      7254|Category_7254|            2434|This is a descrip...|               7|2024-09-30 08:43:41|2024-11-15 08:43:41|\n|      7255|Category_7255|            null|This is a descrip...|              70|2024-04-26 08:43:41|2024-11-29 08:43:41|\n|      7256|Category_7256|            7215|This is a descrip...|             365|2024-06-12 08:43:41|2024-11-10 08:43:41|\n|      7257|Category_7257|            null|This is a descrip...|             195|2024-04-05 08:43:41|2024-11-05 08:43:41|\n|      7258|Category_7258|            null|This is a descrip...|             170|2024-08-10 08:43:41|2024-11-08 08:43:41|\n|      7259|Category_7259|            1052|This is a descrip...|             152|2024-04-21 08:43:41|2024-11-04 08:43:41|\n|      7260|Category_7260|            null|This is a descrip...|              14|2024-04-06 08:43:41|2024-11-09 08:43:41|\n|      7261|Category_7261|            2506|This is a descrip...|             457|2024-08-10 08:43:41|2024-11-05 08:43:41|\n|      7262|Category_7262|            null|This is a descrip...|             176|2024-08-02 08:43:41|2024-11-25 08:43:41|\n|      7263|Category_7263|            null|This is a descrip...|              85|2024-03-12 08:43:41|2024-11-30 08:43:41|\n|      7264|Category_7264|            7981|This is a descrip...|              56|2024-06-11 08:43:41|2024-11-21 08:43:41|\n|      7265|Category_7265|            1967|This is a descrip...|             168|2024-04-13 08:43:41|2024-11-19 08:43:41|\n|      7266|Category_7266|            4891|This is a descrip...|             428|2024-09-25 08:43:41|2024-11-11 08:43:41|\n|      7267|Category_7267|            6606|This is a descrip...|             302|2024-10-13 08:43:41|2024-11-27 08:43:41|\n|      7268|Category_7268|            2907|This is a descrip...|             102|2024-09-11 08:43:41|2024-11-09 08:43:41|\n|      7269|Category_7269|            null|This is a descrip...|             351|2024-01-10 08:43:41|2024-11-23 08:43:41|\n+----------+-------------+----------------+--------------------+----------------+-------------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##Limit condition \n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df=df.limit(1000)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c397b3-59fe-4c46-80ab-c224b4490e11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1542f8c8-f1b7-4533-bf37-b218e6c6b504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|Master Category Name|\n+--------------------+\n|       Category_7250|\n|       Category_7251|\n|       Category_7252|\n|       Category_7253|\n|       Category_7254|\n|       Category_7255|\n|       Category_7256|\n|       Category_7257|\n|       Category_7258|\n|       Category_7259|\n|       Category_7260|\n|       Category_7261|\n|       Category_7262|\n|       Category_7263|\n|       Category_7264|\n|       Category_7265|\n|       Category_7266|\n|       Category_7267|\n|       Category_7268|\n|       Category_7269|\n+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##Alias condition \n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df=df.select(col('CategoryName').alias(\"Master Category Name\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bce5a1a9-5a27-45bd-a7f6-db60d7a1b3e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908931fa-3690-4a85-81f6-1cacdde872a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------------+--------------------+----------------+-------------------+-------------------+-----------------+\n|CategoryID| CategoryName|ParentCategoryID|         Description|NumberOfProducts|        CreatedDate|        LastUpdated|NumberOfProducts2|\n+----------+-------------+----------------+--------------------+----------------+-------------------+-------------------+-----------------+\n|      7250|Category_7250|            5132|This is a descrip...|             414|2024-01-01 08:43:41|2024-11-25 08:43:41|              828|\n|      7251|Category_7251|            1793|This is a descrip...|              79|2024-08-26 08:43:41|2024-11-02 08:43:41|              158|\n|      7252|Category_7252|            5501|This is a descrip...|             414|2024-11-21 08:43:41|2024-11-07 08:43:41|              828|\n|      7253|Category_7253|            2682|This is a descrip...|              21|2024-03-26 08:43:41|2024-11-10 08:43:41|               42|\n|      7254|Category_7254|            2434|This is a descrip...|               7|2024-09-30 08:43:41|2024-11-15 08:43:41|               14|\n|      7255|Category_7255|            null|This is a descrip...|              70|2024-04-26 08:43:41|2024-11-29 08:43:41|              140|\n|      7256|Category_7256|            7215|This is a descrip...|             365|2024-06-12 08:43:41|2024-11-10 08:43:41|              730|\n|      7257|Category_7257|            null|This is a descrip...|             195|2024-04-05 08:43:41|2024-11-05 08:43:41|              390|\n|      7258|Category_7258|            null|This is a descrip...|             170|2024-08-10 08:43:41|2024-11-08 08:43:41|              340|\n|      7259|Category_7259|            1052|This is a descrip...|             152|2024-04-21 08:43:41|2024-11-04 08:43:41|              304|\n|      7260|Category_7260|            null|This is a descrip...|              14|2024-04-06 08:43:41|2024-11-09 08:43:41|               28|\n|      7261|Category_7261|            2506|This is a descrip...|             457|2024-08-10 08:43:41|2024-11-05 08:43:41|              914|\n|      7262|Category_7262|            null|This is a descrip...|             176|2024-08-02 08:43:41|2024-11-25 08:43:41|              352|\n|      7263|Category_7263|            null|This is a descrip...|              85|2024-03-12 08:43:41|2024-11-30 08:43:41|              170|\n|      7264|Category_7264|            7981|This is a descrip...|              56|2024-06-11 08:43:41|2024-11-21 08:43:41|              112|\n|      7265|Category_7265|            1967|This is a descrip...|             168|2024-04-13 08:43:41|2024-11-19 08:43:41|              336|\n|      7266|Category_7266|            4891|This is a descrip...|             428|2024-09-25 08:43:41|2024-11-11 08:43:41|              856|\n|      7267|Category_7267|            6606|This is a descrip...|             302|2024-10-13 08:43:41|2024-11-27 08:43:41|              604|\n|      7268|Category_7268|            2907|This is a descrip...|             102|2024-09-11 08:43:41|2024-11-09 08:43:41|              204|\n|      7269|Category_7269|            null|This is a descrip...|             351|2024-01-10 08:43:41|2024-11-23 08:43:41|              702|\n+----------+-------------+----------------+--------------------+----------------+-------------------+-------------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##selectExpr condition \n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df=df.selectExpr(\"*\",\"NumberOfProducts*2 as NumberOfProducts2\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa027fe-cb94-4c7b-9d2d-de7d0d0cc945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(1) Project [CategoryID#663, CategoryName#664, ParentCategoryID#665, Description#666, NumberOfProducts#667, CreatedDate#668, LastUpdated#669, (NumberOfProducts#667 * 2) AS NumberOfProducts2#677]\n+- *(1) ColumnarToRow\n   +- FileScan parquet [CategoryID#663,CategoryName#664,ParentCategoryID#665,Description#666,NumberOfProducts#667,CreatedDate#668,LastUpdated#669] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/adls/target_tables/Dim/categories_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<CategoryID:int,CategoryName:string,ParentCategoryID:int,Description:string,NumberOfProduct...\n\n\n"
     ]
    }
   ],
   "source": [
    "##execution plan \n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df=df.selectExpr(\"*\",\"NumberOfProducts*2 as NumberOfProducts2\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b1698e-d445-4fd3-a496-cc6df998eec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9da3439-1911-4fa3-af57-165d55b5f197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Extra Union condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4dc0131-022e-4caa-9d05-58437d246d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+\n|CategoryID|ParentCategoryID|NumberOfProducts|\n+----------+----------------+----------------+\n+----------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "##Union\n",
    "# single cell execution\n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df_1=df.select(df.CategoryID, df.ParentCategoryID, df.NumberOfProducts).filter(df.ParentCategoryID=='1520')\n",
    "df_2=df.select(df.CategoryID, df.ParentCategoryID, df.NumberOfProducts).filter(df.ParentCategoryID=='3071')\n",
    "union_df=df_1.union(df_2)\n",
    "union_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb128b0-2a24-48d2-968a-52d8b8b4f49c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+\n|CategoryID|ParentCategoryID|NumberOfProducts|\n+----------+----------------+----------------+\n+----------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "##Union\n",
    "# Multi cell execution\n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df_1=df.select(df.CategoryID, df.ParentCategoryID, df.NumberOfProducts).filter(df.ParentCategoryID=='3071')\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40761d91-aee5-4c1a-aca1-5fcf2d533319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+\n|CategoryID|ParentCategoryID|NumberOfProducts|\n+----------+----------------+----------------+\n+----------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "##Union\n",
    "# Multi cell execution\n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df_2=df.select(df.CategoryID, df.ParentCategoryID, df.NumberOfProducts).filter(df.ParentCategoryID=='3071')\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ab1ad0-01cc-4006-9930-815c503a025d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+\n|CategoryID|ParentCategoryID|NumberOfProducts|\n+----------+----------------+----------------+\n+----------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "##Union\n",
    "# Multi cell execution\n",
    "union_df=df_1.union(df_2)\n",
    "union_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e49239c-9cb2-4ef6-8fcb-8cec3c652eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Observation:\n",
    "#### Query Execution Behavior in DAG\n",
    "\n",
    "While executing the `UNION` command in a single cell, if we check the DAG image, we can see that for **Filter 3**, we get 1 record as output, and for **Filter 8**, we get 0 records. Despite this, we still get **2 records** as output, which is the expected result.\n",
    "\n",
    "However, if you execute all the commands in separate cells, the DAG image shows **1 record** for each of the filters. This discrepancy occurs due to improper logging happening at the backend. As a result, while the output and the jobs will not differ between both cases, the logging behavior leads to different interpretations of the DAG.\n",
    "\n",
    "\n",
    "#####Note:\n",
    "In data frame point of view Union comes under wide tranformation as the partition vlaues changes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2227e6-e03f-48d0-a9bd-7191a80afd62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Wide Transformations\n",
    "\n",
    "Wide transformations in PySpark are operations that require **shuffling data across partitions**. This means that the data needs to be moved between executor or worker nodes to perform the transformation.\n",
    "\n",
    "These transformations are generally more expensive in terms of computation and network I/O because they involve redistributing data to ensure that rows with the same key or required data are grouped together. This often results in a high cost for large datasets.\n",
    "\n",
    "### Some examples of wide transformations in Spark include:\n",
    "\n",
    "- **groupBy**: Groups the data by a specified column or columns.\n",
    "- **groupByKey()**: Groups the data by key (for RDD operations).\n",
    "- **reduceByKey()**: Reduces the data by key using a given function.\n",
    "- **aggregate()**: Aggregates the data using an initial value and a function, producing a final result.\n",
    "- **aggregateByKey()**: Similar to `reduceByKey`, but allows for a more complex aggregation using separate functions for combining values within partitions and across partitions.\n",
    "- **distinct()**: Removes duplicate values from the dataset, requiring all data to be shuffled to identify unique rows.\n",
    "- **join()**: Joins two datasets based on a common key, which involves shuffling data to align the rows with the same key from both datasets.\n",
    "- **repartition()**: Reshuffles the data into a specified number of partitions, requiring data to be redistributed across the cluster.\n",
    "\n",
    "### Example of Wide Transformations\n",
    "\n",
    "#### 1. `groupBy`\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Wide Transformation Example\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Alice\", \"HR\", 1000),\n",
    "        (\"Bob\", \"Finance\", 1500),\n",
    "        (\"Alice\", \"HR\", 1100),\n",
    "        (\"Charlie\", \"Finance\", 2000),\n",
    "        (\"Charlie\", \"HR\", 1200)]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"name\", \"department\", \"salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Perform a wide transformation with groupBy\n",
    "df_grouped = df.groupBy(\"name\").agg({\"salary\": \"avg\"})\n",
    "\n",
    "# Show the result of groupBy transformation\n",
    "df_grouped.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18ec126-39e2-4699-bff2-f929064e717a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Wide Transformations Table\n",
    "\n",
    "| Transformation        | Description                                                                                                     |\n",
    "|-----------------------|-----------------------------------------------------------------------------------------------------------------|\n",
    "| **groupBy**           | Groups the data by a specified column or columns. This operation requires shuffling data across partitions.    |\n",
    "| **groupByKey()**      | Groups the data by key (for RDD operations). Requires shuffling data for proper key grouping.                 |\n",
    "| **reduceByKey()**     | Reduces the data by key using a specified function. Shuffles data across partitions to aggregate values.       |\n",
    "| **aggregate()**       | Aggregates the data using an initial value and a function. Results in a final aggregated value.                |\n",
    "| **aggregateByKey()**  | Similar to `reduceByKey()`, but allows for more complex aggregation with different functions for combining values within partitions and across partitions. |\n",
    "| **distinct()**        | Removes duplicate values from the dataset, requiring a shuffle to identify unique rows across all partitions. |\n",
    "| **join()**            | Joins two datasets based on a common key, requiring data to be shuffled to align rows with the same key.       |\n",
    "| **repartition()**     | Reshuffles the data into a specified number of partitions, requiring data redistribution across the cluster.    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f41facd-fd29-4710-a576-8999c947b336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n|ParentCategoryID|NumberOfProducts|\n+----------------+----------------+\n|            3794|             211|\n|            4101|             344|\n|            1829|             555|\n|            5518|             255|\n|            1591|              43|\n|            9427|             293|\n|            4900|             495|\n|            1342|              39|\n|            6397|             207|\n|            9900|             413|\n|            1088|             455|\n|            7340|             303|\n|            8086|              76|\n|            5614|             880|\n|            1395|              27|\n|            8932|             214|\n|            6393|             421|\n|            7417|             305|\n|            4161|             177|\n|            1896|             120|\n+----------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##groupBy\n",
    "df=spark.read.parquet(\"dbfs:/mnt/adls/target_tables/Dim/categories_table/\")\n",
    "df = df.groupBy(\"ParentCategoryID\").agg(sum(col(\"NumberOfProducts\").cast(IntegerType())).alias(\"NumberOfProducts\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3af8bbc-6ed2-47f0-9107-5e4fefcf5c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Shuffle Read and Shuffle Write in Apache Spark\n",
    "\n",
    "In distributed computing frameworks like Apache Spark, **shuffle** is a critical concept that directly affects performance. It occurs when Spark needs to reorganize or redistribute data across different nodes or partitions, often due to operations like `groupBy`, `join`, or `repartition`. These operations require data to be moved across the network between workers, and this process involves both **Shuffle Read** and **Shuffle Write**.\n",
    "\n",
    "### 1. Shuffle Read\n",
    "\n",
    "**Shuffle Read** refers to the amount of data read by tasks during the shuffle process.\n",
    "\n",
    "### What happens during a shuffle?\n",
    "- When Spark executes operations like `groupBy`, `join`, or `repartition`, it might need to redistribute data across multiple workers (nodes) in the cluster. \n",
    "- For example, in a `groupBy` operation, Spark needs to collect all the data related to a specific key (e.g., `ParentCategoryID` in your case) on the same node to perform the aggregation. As a result, data from different partitions (which might be on different workers) must be moved or \"shuffled\" to the correct workers.\n",
    "\n",
    "### How does it work?\n",
    "- Suppose you have a dataset that is divided into multiple partitions across nodes. If you're performing a `groupBy` operation, Spark will redistribute the data so that all records belonging to the same `ParentCategoryID` are grouped together. For this to happen, Spark needs to **read** data from other partitions and bring it to the node where it will be processed.\n",
    "\n",
    "### Why does Shuffle Read matter?\n",
    "- **Network and Disk I/O**: Shuffle involves significant network and disk input/output (I/O) because data has to be transferred between nodes and potentially written to disk temporarily. The more data being shuffled, the higher the cost in terms of network and disk bandwidth. This can lead to slow performance, especially when large datasets are involved.\n",
    "- **Performance Bottleneck**: If a large amount of data is being shuffled, it can create a bottleneck, making the process slower. Network congestion and high disk usage during shuffle can lead to delays, increased latency, and overall inefficiency.\n",
    "- **Shuffle Read Size**: The **Shuffle Read** value tells you how much data was read from other partitions or nodes during this redistribution. A large **Shuffle Read** size could indicate a very expensive operation, especially if Spark needs to shuffle a lot of data.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Shuffle Write\n",
    "\n",
    "**Shuffle Write** refers to the amount of data written by tasks to temporary storage during the shuffle process.\n",
    "\n",
    "### What happens during shuffle write?\n",
    "- After Spark redistributes the data (as part of the shuffle), it writes this data to temporary disk storage (on each node) so that tasks on other nodes can access it.\n",
    "- For example, if a task needs data from other partitions, it will read the required data from a disk (this is **Shuffle Read**). After performing its local computation (e.g., a `groupBy` aggregation), Spark will write the processed data to disk (this is **Shuffle Write**) so that other tasks can access it and complete their computation.\n",
    "\n",
    "### How does it work?\n",
    "- After the data is read and processed locally, Spark writes the results to disk in the form of intermediate files. These files are stored temporarily in Spark’s shuffle buffer (disk storage), which might involve writing to local disks or distributed file systems like HDFS, depending on the configuration.\n",
    "- The amount of data written to disk depends on how much data is being shuffled and how Spark decides to partition the results (i.e., how the data is redistributed).\n",
    "\n",
    "### Why does Shuffle Write matter?\n",
    "- **Disk and Network I/O**: Shuffle Write also involves significant disk and network I/O, as the data is written to intermediate storage locations and might be transferred across the network for future tasks.\n",
    "- **Temporary Storage**: Shuffle Write is typically written to temporary files, which means these operations can consume a lot of disk space and I/O bandwidth, leading to potential bottlenecks.\n",
    "- **Performance Impact**: Writing large volumes of data during shuffle operations increases the overall time it takes for the job to finish. This is especially problematic when you have a high volume of intermediate data, as it leads to disk contention and slower performance.\n",
    "- **Shuffle Write Size**: The **Shuffle Write** value indicates how much data Spark has written during the shuffle phase. A large **Shuffle Write** value suggests that a lot of data is being transferred and stored temporarily, which may be indicative of inefficient partitioning or too much data being shuffled.\n",
    "\n",
    "---\n",
    "\n",
    "## How Shuffle Read and Shuffle Write Affect Performance\n",
    "\n",
    "1. **Network and Disk Overhead**:\n",
    "   - Both Shuffle Read and Shuffle Write involve significant network and disk I/O. Since Spark is a distributed system, data needs to be moved between nodes in the cluster during the shuffle. This can cause high network traffic, leading to delays and inefficiencies, especially when the cluster is under heavy load.\n",
    "   - If there is a lot of shuffle traffic (both reading and writing), it can saturate the network and disk, causing slowdowns.\n",
    "\n",
    "2. **Data Partitioning**:\n",
    "   - If the data is not partitioned optimally, Spark might need to shuffle large amounts of data. For example, if your data is heavily skewed, certain partitions may have far more data than others, requiring additional shuffling and increasing Shuffle Read/Write.\n",
    "   - **Repartitioning** or **coalescing** the data (to adjust the number of partitions) can help reduce shuffle costs.\n",
    "\n",
    "3. **Skew in Data**:\n",
    "   - If one partition has significantly more data than others (known as data skew), Spark will need to shuffle more data to balance the workload across nodes. This can lead to increased Shuffle Read/Write, as Spark redistributes the data to other workers.\n",
    "   - Skewed data can also lead to long-running tasks that create a bottleneck in the shuffle process, affecting overall job performance.\n",
    "\n",
    "4. **Job Optimization**:\n",
    "   - To optimize jobs that involve shuffling, you can:\n",
    "     - **Increase parallelism** by adjusting the number of partitions (using `repartition()` or `coalesce()`).\n",
    "     - **Avoid unnecessary shuffles**: Try to design the workflow to reduce unnecessary shuffling, such as using `map` and `filter` operations before `groupBy` to reduce the amount of data being shuffled.\n",
    "     - **Broadcast joins**: In the case of joins, use broadcast joins if one of the datasets is small enough to be broadcast to all nodes, thereby avoiding a shuffle.\n",
    "\n",
    "5. **Tuning**:\n",
    "   - **Memory settings**: Ensure that the memory settings for your Spark job are adequate, especially for tasks that involve heavy shuffling. If there isn’t enough memory, Spark might spill data to disk, which can slow down the shuffle process.\n",
    "   - **Shuffle Partition Size**: You can tune the number of shuffle partitions (`spark.sql.shuffle.partitions`) to control how data is distributed during the shuffle. Having too many partitions can result in excessive shuffle, while too few partitions can lead to large partitions with inefficient processing.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Shuffle Read** represents the data being read from other partitions during the shuffle process, while **Shuffle Write** represents the data being written during the same process.\n",
    "- Both processes involve significant disk and network I/O, and understanding how much data is shuffled can help in identifying performance bottlenecks.\n",
    "- Large shuffle sizes (both read and write) can indicate inefficient partitioning or data skew, which may require optimizations such as adjusting the number of partitions, repartitioning the data, or avoiding unnecessary shuffling.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 905072674377608,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Narrow and Wide Transformations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
